{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis\n",
      "  Cloning https://github.com/oshadura/topcoffea.git (to revision coffea-casa-analysis) to /tmp/pip-req-build-7h8w9ss_\n",
      "Requirement already satisfied (use --upgrade to upgrade): topcoffea==0.0.0 from git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis in /opt/conda/lib/python3.8/site-packages\n",
      "Building wheels for collected packages: topcoffea\n",
      "  Building wheel for topcoffea (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for topcoffea: filename=topcoffea-0.0.0-py3-none-any.whl size=4513687 sha256=ef030747c7007667a3b9b8a7bc13d11c944b76e4cdad7aab00da5cbce93236ed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0j5azoud/wheels/3a/95/24/bfbb2d1dc4571114a0106e7d0d6a8124bcd2ad9e72ec9ea14e\n",
      "Successfully built topcoffea\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: awkward==1.3.0 in /opt/conda/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from awkward==1.3.0) (57.0.0)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.8/site-packages (from awkward==1.3.0) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis\n",
    "! pip install awkward==1.3.0\n",
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame as lz4f\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import cloudpickle\n",
    "import gzip\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "from coffea import hist, processor\n",
    "from coffea.util import load, save\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "\n",
    "from topcoffea.modules import samples\n",
    "from topcoffea.modules import fileReader\n",
    "\n",
    "import sys\n",
    "from analysis.topEFT import topeft\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    import sys\n",
    "    sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonFile was selected for UNL /home/cms-jovyan/topcoffea/topcoffea/json/TTZToLLNuNu_M10.json\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='You can customize your run')\n",
    "parser.add_argument('jsonFiles'           , nargs='?', help = 'Json file(s) containing files and metadata')\n",
    "parser.add_argument('--prefix', '-r'     , nargs='?', help = 'Prefix or redirector to look for the files')\n",
    "parser.add_argument('--test','-t'       , action='store_true'  , help = 'To perform a test, run over a few events in a couple of chunks')\n",
    "parser.add_argument('--pretend'        , action='store_true'  , help = 'Read json files but, not execute the analysis')\n",
    "#parser.add_argument('--nworkers','-n'   , default=8  , help = 'Number of workers')\n",
    "parser.add_argument('--chunksize','-s'   , default=500000  , help = 'Number of events per chunk')\n",
    "parser.add_argument('--nchunks','-c'   , default=None  , help = 'You can choose to run only a number of chunks')\n",
    "parser.add_argument('--outname','-o'   , default='plotsTopEFT', help = 'Name of the output file with histograms')\n",
    "parser.add_argument('--outpath','-p'   , default='histos', help = 'Name of the output directory')\n",
    "parser.add_argument('--treename'   , default='Events', help = 'Name of the tree inside the files')\n",
    "parser.add_argument('--do-errors', action='store_true', help = 'Save the w**2 coefficients')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.jsonFiles is not None:\n",
    "  jsonFiles    = args.jsonFiles\n",
    "  print('jsonFiles {}'.format(args.jsonFiles))\n",
    "else:\n",
    "  with importlib.resources.path(\"topcoffea.json\", \"TTZToLLNuNu_M10.json\") as path:\n",
    "    jsonFiles = str(path)\n",
    "    print('jsonFile was selected for UNL {}'.format(jsonFiles))\n",
    "    \n",
    "if args.prefix is not None:\n",
    "  prefix    = args.prefix\n",
    "else:\n",
    "  prefix = \"root://xcache//\"\n",
    "\n",
    "dotest     = args.test\n",
    "#nworkers   = int(args.nworkers)\n",
    "chunksize  = int(args.chunksize)\n",
    "nchunks    = int(args.nchunks) if not args.nchunks is None else args.nchunks\n",
    "outname    = args.outname\n",
    "outpath    = args.outpath\n",
    "pretend    = args.pretend\n",
    "treename   = args.treename\n",
    "do_errors = args.do_errors\n",
    "\n",
    "if dotest:\n",
    "  nchunks = 2\n",
    "  chunksize = 10000\n",
    "  nworkers = 1\n",
    "  print('Running a fast test with %i workers, %i chunks of %i events'%(nworkers, nchunks, chunksize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cms-jovyan/topcoffea/topcoffea/json/TTZToLLNuNu_M10.json']\n",
      ">> TTZToLLNuNu_M10\n",
      "   - isData?      : NO\n",
      "   - year         : 2018\n",
      "   - xsec         : 0.252900\n",
      "   - histAxisName : \n",
      "   - options      : \n",
      "   - tree         : Events\n",
      "   - nEvents      : 14542666\n",
      "   - nGenEvents   : 4876491\n",
      "   - SumWeights   : 19992000.000000\n",
      "   - Prefix       : root://xcache//\n",
      "   - nFiles       : 18\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_1.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_10.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_11.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_12.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_13.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_14.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_15.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_16.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_17.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_18.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_2.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_3.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_4.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_5.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_6.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_7.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_8.root\n",
      "     /store/user/jrgonzal/nanoAODcrab/TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/mc2018_28apr2021_TTZToLLNuNu_M-10_TuneCP5_13TeV-amcatnlo-pythia8/210427_230816/0000/tree_9.root\n"
     ]
    }
   ],
   "source": [
    "samplesdict = {}\n",
    "allInputFiles = []\n",
    "\n",
    "def LoadJsonToSampleName(jsonFile, prefix):\n",
    "  sampleName = jsonFile if not '/' in jsonFile else jsonFile[jsonFile.rfind('/')+1:]\n",
    "  if sampleName.endswith('.json'): sampleName = sampleName[:-5]\n",
    "  with open(jsonFile) as jf:\n",
    "    samplesdict[sampleName] = json.load(jf)\n",
    "    samplesdict[sampleName]['redirector'] = prefix\n",
    "\n",
    "if  isinstance(jsonFiles, str) and ',' in jsonFiles: jsonFiles = jsonFiles.replace(' ', '').split(',')\n",
    "elif isinstance(jsonFiles, str)                     : jsonFiles = [jsonFiles]\n",
    "\n",
    "for jsonFile in jsonFiles:\n",
    "  if os.path.isdir(jsonFile):\n",
    "    if not jsonFile.endswith('/'): jsonFile+='/'\n",
    "    for f in os.path.listdir(jsonFile):\n",
    "      if f.endswith('.json'): allInputFiles.append(jsonFile+f)\n",
    "  else:\n",
    "    allInputFiles.append(jsonFile)\n",
    "\n",
    "print(allInputFiles)\n",
    "# Read from cfg files\n",
    "for f in allInputFiles:\n",
    "  if not os.path.isfile(f):\n",
    "    print('[WARNING] Input file \"%s% not found!'%f)\n",
    "    continue\n",
    "  # This input file is a json file, not a cfg\n",
    "  if f.endswith('.json'): \n",
    "    LoadJsonToSampleName(f, prefix)\n",
    "  # Open cfg files\n",
    "  else:\n",
    "    with open(f) as fin:\n",
    "      print(' >> Reading json from cfg file...')\n",
    "      lines = fin.readlines()\n",
    "      for l in lines:\n",
    "        if '#' in l: l=l[:l.find('#')]\n",
    "        l = l.replace(' ', '').replace('\\n', '')\n",
    "        if l == '': continue\n",
    "        if ',' in l:\n",
    "          l = l.split(',')\n",
    "          for nl in l:\n",
    "            if not os.path.isfile(l): prefix = nl\n",
    "            else: LoadJsonToSampleName(nl, prefix)\n",
    "        else:\n",
    "          if not os.path.isfile(l): prefix = l\n",
    "          else: LoadJsonToSampleName(l, prefix)\n",
    "\n",
    "flist = {};\n",
    "for sname in samplesdict.keys():\n",
    "  redirector = samplesdict[sname]['redirector']\n",
    "  flist[sname] = [(redirector+f) for f in samplesdict[sname]['files']]\n",
    "  samplesdict[sname]['year'] = int(samplesdict[sname]['year'])\n",
    "  samplesdict[sname]['xsec'] = float(samplesdict[sname]['xsec'])\n",
    "  samplesdict[sname]['nEvents'] = int(samplesdict[sname]['nEvents'])\n",
    "  samplesdict[sname]['nGenEvents'] = int(samplesdict[sname]['nGenEvents'])\n",
    "  samplesdict[sname]['nSumOfWeights'] = float(samplesdict[sname]['nSumOfWeights'])\n",
    "\n",
    "  # Print file info\n",
    "  print('>> '+sname)\n",
    "  print('   - isData?      : %s'   %('YES' if samplesdict[sname]['isData'] else 'NO'))\n",
    "  print('   - year         : %i'   %samplesdict[sname]['year'])\n",
    "  print('   - xsec         : %f'   %samplesdict[sname]['xsec'])\n",
    "  print('   - histAxisName : %s'   %samplesdict[sname]['histAxisName'])\n",
    "  print('   - options      : %s'   %samplesdict[sname]['options'])\n",
    "  print('   - tree         : %s'   %samplesdict[sname]['treeName'])\n",
    "  print('   - nEvents      : %i'   %samplesdict[sname]['nEvents'])\n",
    "  print('   - nGenEvents   : %i'   %samplesdict[sname]['nGenEvents'])\n",
    "  print('   - SumWeights   : %f'   %samplesdict[sname]['nSumOfWeights'])\n",
    "  print('   - Prefix       : %s'   %samplesdict[sname]['redirector'])\n",
    "  print('   - nFiles       : %i'   %len(samplesdict[sname]['files']))\n",
    "  for fname in samplesdict[sname]['files']: print('     %s'%fname)\n",
    "\n",
    "if pretend: \n",
    "  print('pretending...')\n",
    "  exit() \n",
    "\n",
    "# Check that all datasets have the same list of WCs\n",
    "for i,k in enumerate(samplesdict.keys()):\n",
    "  if i == 0:\n",
    "    wc_lst = samplesdict[k]['WCnames']\n",
    "  if wc_lst != samplesdict[k]['WCnames']:\n",
    "    raise Exception(\"Not all of the datasets have the same list of WCs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 2% Completed | 57.6s6s\r"
     ]
    },
    {
     "ename": "ZipImportError",
     "evalue": "bad local file header: '/home/cms-jovyan/dask-worker-space/worker-4w0itrwe/analysis.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZipImportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4fc3e99b5096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Run the processor and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m output = processor.run_uproot_job(flist,\n\u001b[0m\u001b[1;32m     31\u001b[0m                                   \u001b[0mtreename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtreename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                   \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     }\n\u001b[1;32m   1234\u001b[0m     \u001b[0mexe_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m     \u001b[0mwrapped_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mdask_executor\u001b[0;34m(items, function, accumulator, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;31m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# no @wraps due to pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_compress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m_work_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessorABC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0mprocessor_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlz4f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0mretry_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<frozen zipimport>\u001b[0m in \u001b[0;36mload_module\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<frozen zipimport>\u001b[0m in \u001b[0;36m_get_module_code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<frozen zipimport>\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mZipImportError\u001b[0m: bad local file header: '/home/cms-jovyan/dask-worker-space/worker-4w0itrwe/analysis.zip'"
     ]
    }
   ],
   "source": [
    "processor_instance = topeft.AnalysisProcessor(samplesdict,wc_lst,do_errors)\n",
    "\n",
    "from dask.distributed import Client, Worker, WorkerPlugin\n",
    "import os, shutil\n",
    "from typing import List\n",
    "class DependencyInstaller(WorkerPlugin):\n",
    "    def __init__(self, dependencies: List[str]):\n",
    "        self._depencendies = \" \".join(f\"'{dep}'\" for dep in dependencies)\n",
    "    def setup(self, worker: Worker):\n",
    "        os.system(f\"pip install {self._depencendies}\")\n",
    "dependency_installer = DependencyInstaller([\n",
    "    \"git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis\",\"awkward==1.3.0\"\n",
    "])\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "client.register_worker_plugin(dependency_installer)\n",
    "\n",
    "shutil.make_archive(\"analysis\", \"zip\", base_dir=\"analysis\")\n",
    "client.upload_file(\"analysis.zip\")\n",
    "\n",
    "executor_args = {\n",
    "                 'schema': NanoAODSchema,\n",
    "                 'client': client,\n",
    "                 'savemetrics': True\n",
    "}\n",
    "\n",
    "\n",
    "# Run the processor and get the output                                                                                                                                                                     \n",
    "tic = time.time()\n",
    "output = processor.run_uproot_job(flist,\n",
    "                                  treename=treename,\n",
    "                                  processor_instance=processor_instance,\n",
    "                                  executor=processor.dask_executor,\n",
    "                                  executor_args=executor_args,\n",
    "                                  chunksize=chunksize,\n",
    "                                  maxchunks=nchunks\n",
    "                                 )\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Dask client:\", client)\n",
    "print(\"Total time: %.0f\" % (toc - tic))\n",
    "print(\"Events / s / thread: {:,.0f}\".format(output[1]['entries'] / output[1]['processtime']))\n",
    "print(\"Events / s: {:,.0f}\".format(output[1]['entries'] / (toc - tic)))\n",
    "\n",
    "os.system(\"mkdir -p histos/\")\n",
    "print('Saving output in %s...'%(\"histos/\" + outname + \".pkl.gz\"))\n",
    "with gzip.open(\"histos/\" + outname + \".pkl.gz\", \"wb\") as fout:\n",
    "    cloudpickle.dump(output, fout)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, OrderedDict\n",
    "import gzip\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from coffea import hist, processor\n",
    "from coffea.hist import plot\n",
    "from cycler import cycler\n",
    "\n",
    "from topcoffea.plotter.OutText import OutText\n",
    "\n",
    "\n",
    "path = 'histos/plotsTopEFT.pkl.gz'\n",
    "outname = 'temp.png'\n",
    "\n",
    "# Select variable, channel and cuts\n",
    "var = 'met'\n",
    "channel = ['eemSSonZ', 'eemSSoffZ', 'mmeSSonZ', 'mmeSSoffZ','eeeSSonZ', 'eeeSSoffZ', 'mmmSSonZ', 'mmmSSoffZ']\n",
    "cut = 'base'\n",
    "\n",
    "print('Opening path: ', path)\n",
    "hists = {}\n",
    "with gzip.open(path) as fin:\n",
    "  hin = pickle.load(fin)\n",
    "  print(' >> looking for histograms...')\n",
    "  for k in hin.keys():\n",
    "    if k in hists: hists[k]+=hin[k]\n",
    "    else:          hists[k]=hin[k]\n",
    "\n",
    "\n",
    "# Create figure\n",
    "fig, (ax, rax) = plt.subplots(2, 1, figsize=(14,7), gridspec_kw={\"height_ratios\": (3, 1)}, sharex=True)\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9)\n",
    "\n",
    "# Select the histogram var, channel and cut\n",
    "h = hists[var]\n",
    "h = h.integrate('channel', channel)\n",
    "h = h.integrate('cut', cut)\n",
    "\n",
    "# Integrate over samples\n",
    "h = h.sum('sample')\n",
    " \n",
    "# Plot and save figure to outname\n",
    "hist.plot1d(h, ax=ax, line_opts={'color':'orange'})\n",
    "fig.savefig(outname)\n",
    "print('Output histogram saved in %s'%outname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
