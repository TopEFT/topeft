{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis\n",
    "! pip install awkward==1.3.0\n",
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame as lz4f\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import cloudpickle\n",
    "import gzip\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "from coffea import hist, processor\n",
    "from coffea.util import load, save\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "\n",
    "from topcoffea.modules import samples\n",
    "from topcoffea.modules import fileReader\n",
    "\n",
    "#FIXME: analysis is not installed anywhere (should be installed as well)\n",
    "import topcoffea.analysis.topEFT.topeft\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    import sys\n",
    "    sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='You can customize your run')\n",
    "parser.add_argument('jsonFiles'           , nargs='?', help = 'Json file(s) containing files and metadata')\n",
    "parser.add_argument('--prefix', '-r'     , nargs='?', help = 'Prefix or redirector to look for the files')\n",
    "parser.add_argument('--test','-t'       , action='store_true'  , help = 'To perform a test, run over a few events in a couple of chunks')\n",
    "parser.add_argument('--pretend'        , action='store_true'  , help = 'Read json files but, not execute the analysis')\n",
    "#parser.add_argument('--nworkers','-n'   , default=8  , help = 'Number of workers')\n",
    "parser.add_argument('--chunksize','-s'   , default=500000  , help = 'Number of events per chunk')\n",
    "parser.add_argument('--nchunks','-c'   , default=None  , help = 'You can choose to run only a number of chunks')\n",
    "parser.add_argument('--outname','-o'   , default='plotsTopEFT', help = 'Name of the output file with histograms')\n",
    "parser.add_argument('--outpath','-p'   , default='histos', help = 'Name of the output directory')\n",
    "parser.add_argument('--treename'   , default='Events', help = 'Name of the tree inside the files')\n",
    "parser.add_argument('--do-errors', action='store_true', help = 'Save the w**2 coefficients')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.jsonFiles is not None:\n",
    "  jsonFiles    = args.jsonFiles\n",
    "  print('jsonFiles {}'.format(args.jsonFiles))\n",
    "else:\n",
    "  with importlib.resources.path(\"topcoffea.json\", \"TTZToLLNuNu_M10.json\") as path:\n",
    "    jsonFiles = str(path)\n",
    "    print('jsonFile was selected for UNL {}'.format(jsonFiles))\n",
    "    \n",
    "if args.prefix is not None:\n",
    "  prefix    = args.prefix\n",
    "else:\n",
    "  prefix = \"root://xcache//\"\n",
    "\n",
    "dotest     = args.test\n",
    "#nworkers   = int(args.nworkers)\n",
    "chunksize  = int(args.chunksize)\n",
    "nchunks    = int(args.nchunks) if not args.nchunks is None else args.nchunks\n",
    "outname    = args.outname\n",
    "outpath    = args.outpath\n",
    "pretend    = args.pretend\n",
    "treename   = args.treename\n",
    "do_errors = args.do_errors\n",
    "\n",
    "if dotest:\n",
    "  nchunks = 2\n",
    "  chunksize = 10000\n",
    "  nworkers = 1\n",
    "  print('Running a fast test with %i workers, %i chunks of %i events'%(nworkers, nchunks, chunksize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesdict = {}\n",
    "allInputFiles = []\n",
    "\n",
    "def LoadJsonToSampleName(jsonFile, prefix):\n",
    "  sampleName = jsonFile if not '/' in jsonFile else jsonFile[jsonFile.rfind('/')+1:]\n",
    "  if sampleName.endswith('.json'): sampleName = sampleName[:-5]\n",
    "  with open(jsonFile) as jf:\n",
    "    samplesdict[sampleName] = json.load(jf)\n",
    "    samplesdict[sampleName]['redirector'] = prefix\n",
    "\n",
    "if  isinstance(jsonFiles, str) and ',' in jsonFiles: jsonFiles = jsonFiles.replace(' ', '').split(',')\n",
    "elif isinstance(jsonFiles, str)                     : jsonFiles = [jsonFiles]\n",
    "\n",
    "for jsonFile in jsonFiles:\n",
    "  if os.path.isdir(jsonFile):\n",
    "    if not jsonFile.endswith('/'): jsonFile+='/'\n",
    "    for f in os.path.listdir(jsonFile):\n",
    "      if f.endswith('.json'): allInputFiles.append(jsonFile+f)\n",
    "  else:\n",
    "    allInputFiles.append(jsonFile)\n",
    "\n",
    "print(allInputFiles)\n",
    "# Read from cfg files\n",
    "for f in allInputFiles:\n",
    "  if not os.path.isfile(f):\n",
    "    print('[WARNING] Input file \"%s% not found!'%f)\n",
    "    continue\n",
    "  # This input file is a json file, not a cfg\n",
    "  if f.endswith('.json'): \n",
    "    LoadJsonToSampleName(f, prefix)\n",
    "  # Open cfg files\n",
    "  else:\n",
    "    with open(f) as fin:\n",
    "      print(' >> Reading json from cfg file...')\n",
    "      lines = fin.readlines()\n",
    "      for l in lines:\n",
    "        if '#' in l: l=l[:l.find('#')]\n",
    "        l = l.replace(' ', '').replace('\\n', '')\n",
    "        if l == '': continue\n",
    "        if ',' in l:\n",
    "          l = l.split(',')\n",
    "          for nl in l:\n",
    "            if not os.path.isfile(l): prefix = nl\n",
    "            else: LoadJsonToSampleName(nl, prefix)\n",
    "        else:\n",
    "          if not os.path.isfile(l): prefix = l\n",
    "          else: LoadJsonToSampleName(l, prefix)\n",
    "\n",
    "flist = {};\n",
    "for sname in samplesdict.keys():\n",
    "  redirector = samplesdict[sname]['redirector']\n",
    "  flist[sname] = [(redirector+f) for f in samplesdict[sname]['files']]\n",
    "  samplesdict[sname]['year'] = int(samplesdict[sname]['year'])\n",
    "  samplesdict[sname]['xsec'] = float(samplesdict[sname]['xsec'])\n",
    "  samplesdict[sname]['nEvents'] = int(samplesdict[sname]['nEvents'])\n",
    "  samplesdict[sname]['nGenEvents'] = int(samplesdict[sname]['nGenEvents'])\n",
    "  samplesdict[sname]['nSumOfWeights'] = float(samplesdict[sname]['nSumOfWeights'])\n",
    "\n",
    "  # Print file info\n",
    "  print('>> '+sname)\n",
    "  print('   - isData?      : %s'   %('YES' if samplesdict[sname]['isData'] else 'NO'))\n",
    "  print('   - year         : %i'   %samplesdict[sname]['year'])\n",
    "  print('   - xsec         : %f'   %samplesdict[sname]['xsec'])\n",
    "  print('   - histAxisName : %s'   %samplesdict[sname]['histAxisName'])\n",
    "  print('   - options      : %s'   %samplesdict[sname]['options'])\n",
    "  print('   - tree         : %s'   %samplesdict[sname]['treeName'])\n",
    "  print('   - nEvents      : %i'   %samplesdict[sname]['nEvents'])\n",
    "  print('   - nGenEvents   : %i'   %samplesdict[sname]['nGenEvents'])\n",
    "  print('   - SumWeights   : %f'   %samplesdict[sname]['nSumOfWeights'])\n",
    "  print('   - Prefix       : %s'   %samplesdict[sname]['redirector'])\n",
    "  print('   - nFiles       : %i'   %len(samplesdict[sname]['files']))\n",
    "  for fname in samplesdict[sname]['files']: print('     %s'%fname)\n",
    "\n",
    "if pretend: \n",
    "  print('pretending...')\n",
    "  exit() \n",
    "\n",
    "# Check that all datasets have the same list of WCs\n",
    "for i,k in enumerate(samplesdict.keys()):\n",
    "  if i == 0:\n",
    "    wc_lst = samplesdict[k]['WCnames']\n",
    "  if wc_lst != samplesdict[k]['WCnames']:\n",
    "    raise Exception(\"Not all of the datasets have the same list of WCs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_instance = topcoffea.analysis.topEFT.topeft.AnalysisProcessor(samplesdict,wc_lst,do_errors)\n",
    "\n",
    "from dask.distributed import Client, Worker, WorkerPlugin\n",
    "import os\n",
    "from typing import List\n",
    "class DependencyInstaller(WorkerPlugin):\n",
    "    def __init__(self, dependencies: List[str]):\n",
    "        self._depencendies = \" \".join(f\"'{dep}'\" for dep in dependencies)\n",
    "    def setup(self, worker: Worker):\n",
    "        os.system(f\"pip install {self._depencendies}\")\n",
    "dependency_installer = DependencyInstaller([\n",
    "    \"git+https://github.com/oshadura/topcoffea.git@coffea-casa-analysis\",\"awkward==1.3.0\"\n",
    "])\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "client.register_worker_plugin(dependency_installer)\n",
    "\n",
    "executor_args = {\n",
    "                 'schema': NanoAODSchema,\n",
    "                 'client': client,\n",
    "                 'savemetrics': True\n",
    "}\n",
    "\n",
    "\n",
    "# Run the processor and get the output                                                                                                                                                                     \n",
    "tic = time.time()\n",
    "output = processor.run_uproot_job(flist,\n",
    "                                  treename=treename,\n",
    "                                  processor_instance=processor_instance,\n",
    "                                  executor=processor.dask_executor,\n",
    "                                  executor_args=executor_args,\n",
    "                                  chunksize=chunksize,\n",
    "                                  maxchunks=nchunks\n",
    "                                 )\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Dask client:\", client)\n",
    "print(\"Total time: %.0f\" % (toc - tic))\n",
    "print(\"Events / s / thread: {:,.0f}\".format(output[1]['entries'] / output[1]['processtime']))\n",
    "print(\"Events / s: {:,.0f}\".format(output[1]['entries'] / (toc - tic)))\n",
    "\n",
    "os.system(\"mkdir -p histos/\")\n",
    "print('Saving output in %s...'%(\"histos/\" + outname + \".pkl.gz\"))\n",
    "with gzip.open(\"histos/\" + outname + \".pkl.gz\", \"wb\") as fout:\n",
    "    cloudpickle.dump(output, fout)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, OrderedDict\n",
    "import gzip\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from coffea import hist, processor\n",
    "from coffea.hist import plot\n",
    "from cycler import cycler\n",
    "\n",
    "from topcoffea.plotter.OutText import OutText\n",
    "\n",
    "\n",
    "path = 'histos/plotsTopEFT.pkl.gz'\n",
    "outname = 'temp.png'\n",
    "\n",
    "# Select variable, channel and cuts\n",
    "var = 'met'\n",
    "channel = ['eemSSonZ', 'eemSSoffZ', 'mmeSSonZ', 'mmeSSoffZ','eeeSSonZ', 'eeeSSoffZ', 'mmmSSonZ', 'mmmSSoffZ']\n",
    "cut = 'base'\n",
    "\n",
    "print('Opening path: ', path)\n",
    "hists = {}\n",
    "with gzip.open(path) as fin:\n",
    "  hin = pickle.load(fin)\n",
    "  print(' >> looking for histograms...')\n",
    "  for k in hin.keys():\n",
    "    if k in hists: hists[k]+=hin[k]\n",
    "    else:          hists[k]=hin[k]\n",
    "\n",
    "\n",
    "# Create figure\n",
    "fig, (ax, rax) = plt.subplots(2, 1, figsize=(14,7), gridspec_kw={\"height_ratios\": (3, 1)}, sharex=True)\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9)\n",
    "\n",
    "# Select the histogram var, channel and cut\n",
    "h = hists[var]\n",
    "h = h.integrate('channel', channel)\n",
    "h = h.integrate('cut', cut)\n",
    "\n",
    "# Integrate over samples\n",
    "h = h.sum('sample')\n",
    " \n",
    "# Plot and save figure to outname\n",
    "hist.plot1d(h, ax=ax, line_opts={'color':'orange'})\n",
    "fig.savefig(outname)\n",
    "print('Output histogram saved in %s'%outname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}